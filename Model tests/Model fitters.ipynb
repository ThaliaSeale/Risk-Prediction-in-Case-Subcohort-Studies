{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16635b45-1421-466b-a61c-ab8a0ae5726c",
   "metadata": {},
   "source": [
    "# Linear Model Fits\n",
    "Fitting the models for simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4078f3e-77d1-4ba5-9c14-65470385a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import weibull_min # r weibull simulation\n",
    "from scipy.stats import norm # for covariate simulation\n",
    "from scipy.stats import gamma # for weibull shape parameter\n",
    "from scipy.stats import bernoulli # for censoring\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33bfe5f-2708-4960-9fc9-641a763b4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_simulation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4487e1-79aa-4a36-84b2-666249e47cb7",
   "metadata": {},
   "source": [
    "For writing the functions, we start with a simulated case-subcohort and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382f8b89-bd41-43d8-b1b4-c2661ce14e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>y</th>\n",
       "      <th>end_censor</th>\n",
       "      <th>dropout</th>\n",
       "      <th>end_censor_time</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.196476</td>\n",
       "      <td>0.191833</td>\n",
       "      <td>-0.073135</td>\n",
       "      <td>0.757535</td>\n",
       "      <td>0.061021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.300343</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.300343</td>\n",
       "      <td>0.300343</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.589900</td>\n",
       "      <td>-1.433206</td>\n",
       "      <td>0.065517</td>\n",
       "      <td>0.710613</td>\n",
       "      <td>-0.154888</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925423</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.962799</td>\n",
       "      <td>1.096334</td>\n",
       "      <td>0.070341</td>\n",
       "      <td>-0.014216</td>\n",
       "      <td>0.274551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.267360</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.267360</td>\n",
       "      <td>0.267360</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.833765</td>\n",
       "      <td>3.390531</td>\n",
       "      <td>1.831505</td>\n",
       "      <td>-0.176279</td>\n",
       "      <td>-2.001757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236180</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.236180</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.096859</td>\n",
       "      <td>-0.663814</td>\n",
       "      <td>0.520606</td>\n",
       "      <td>-1.222916</td>\n",
       "      <td>0.353255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.651060</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-0.502243</td>\n",
       "      <td>0.249911</td>\n",
       "      <td>0.212943</td>\n",
       "      <td>1.415288</td>\n",
       "      <td>-0.840517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.628848</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>-1.876473</td>\n",
       "      <td>1.443938</td>\n",
       "      <td>-0.412877</td>\n",
       "      <td>0.444524</td>\n",
       "      <td>-0.583017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.021437</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>-0.400989</td>\n",
       "      <td>0.171076</td>\n",
       "      <td>0.208079</td>\n",
       "      <td>0.950630</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446388</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>-1.047828</td>\n",
       "      <td>1.184693</td>\n",
       "      <td>-0.405803</td>\n",
       "      <td>0.479497</td>\n",
       "      <td>0.592869</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466816</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.278181</td>\n",
       "      <td>-0.647035</td>\n",
       "      <td>-0.486685</td>\n",
       "      <td>1.692078</td>\n",
       "      <td>-0.980449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707747</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>0.425730</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4    5    6    7    8  \\\n",
       "0     1.196476  0.191833 -0.073135  0.757535  0.061021  1.0  1.0  0.0  0.0   \n",
       "1     0.589900 -1.433206  0.065517  0.710613 -0.154888  1.0  0.0  0.0  0.0   \n",
       "2     1.962799  1.096334  0.070341 -0.014216  0.274551  0.0  0.0  1.0  1.0   \n",
       "3     0.833765  3.390531  1.831505 -0.176279 -2.001757  0.0  1.0  1.0  0.0   \n",
       "4    -1.096859 -0.663814  0.520606 -1.222916  0.353255  0.0  1.0  1.0  0.0   \n",
       "...        ...       ...       ...       ...       ...  ...  ...  ...  ...   \n",
       "1495 -0.502243  0.249911  0.212943  1.415288 -0.840517  0.0  0.0  0.0  1.0   \n",
       "1496 -1.876473  1.443938 -0.412877  0.444524 -0.583017  0.0  0.0  1.0  0.0   \n",
       "1497 -0.400989  0.171076  0.208079  0.950630  0.404573  0.0  0.0  1.0  1.0   \n",
       "1498 -1.047828  1.184693 -0.405803  0.479497  0.592869  1.0  0.0  1.0  0.0   \n",
       "1499  0.278181 -0.647035 -0.486685  1.692078 -0.980449  1.0  0.0  0.0  0.0   \n",
       "\n",
       "        9         y  end_censor  dropout  end_censor_time      time  event  \n",
       "0     1.0  0.300343       False    False         0.300343  0.300343   True  \n",
       "1     0.0  0.925423        True    False         0.425730  0.425730  False  \n",
       "2     1.0  0.267360       False    False         0.267360  0.267360   True  \n",
       "3     1.0  0.236180       False     True         0.236180  0.098400  False  \n",
       "4     1.0  0.651060        True    False         0.425730  0.425730  False  \n",
       "...   ...       ...         ...      ...              ...       ...    ...  \n",
       "1495  1.0  0.628848        True    False         0.425730  0.425730  False  \n",
       "1496  0.0  1.021437        True    False         0.425730  0.425730  False  \n",
       "1497  1.0  0.446388        True    False         0.425730  0.425730  False  \n",
       "1498  1.0  0.466816        True    False         0.425730  0.425730  False  \n",
       "1499  1.0  0.707747        True    False         0.425730  0.425730  False  \n",
       "\n",
       "[1500 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covariates = 10\n",
    "sample = weibull_simple_linear_sim([1,1,1,1,1,1,1,1,1,1], 0.5, 1500, 0.7, pi = 0.5)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f31833c-1854-460e-a928-914d03381dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases, subcohort, cohort, test = cch_splitter(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e1fd57-6b2a-45f2-b55a-db7d1c596cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 16), (300, 16), (1000, 16), (500, 16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases.shape, subcohort.shape, cohort.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac467a-68c0-4a89-ae51-008e65d62cb4",
   "metadata": {},
   "source": [
    "## Cox mode, unweighted\n",
    "First we look at the standard Cox model without adjusting for bias. When the model is not mispecified at all, the performance of the unweighted version may not be that different, however, if the model is mispecified, the results may be poorer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5d8cf-842f-48af-8cc1-d6c0eef03c2b",
   "metadata": {},
   "source": [
    "The following function fits an unweighted Cox model for the Weibull simple linear case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b32507c-8ed6-4ca2-9513-7dfa7f7b2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "def fit_cox(cases, subcohort,n_covariates):\n",
    "    # creating a single case subcohort dataframe\n",
    "    case_subcohort_df = pd.concat([cases,subcohort])\n",
    "    # removing unnecessary columns and duplicate rows\n",
    "    case_subcohort_df = case_subcohort_df.loc[case_subcohort_df.duplicated() == False,[i for i in range(0,n_covariates)]+[\"time\", \"event\"]]\n",
    "    \n",
    "    # creating the model and fitting the data\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(case_subcohort_df, duration_col = \"time\", event_col = \"event\")\n",
    "    return(cph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e1810a9-6fb8-4adc-a582-f0198aab51a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'time'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'event'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-1503.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2022-07-31 12:15:09 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 12px;\"></th>\n",
       "      <th style=\"min-width: 12px;\">coef</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">cmp to</th>\n",
       "      <th style=\"min-width: 12px;\">z</th>\n",
       "      <th style=\"min-width: 12px;\">p</th>\n",
       "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.56</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>117.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.77</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.17</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>78.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.30</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>95.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.78</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.91</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.82</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>104.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.84</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.28</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>95.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.83</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.45</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>33.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2.11</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.72</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>46.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.69</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.63</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>25.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.94</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.38</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>42.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.74</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.87</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>27.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><br><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Concordance</th>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>3026.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>485.26 on 10 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>322.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &  coef &  exp(coef) &  se(coef) &  coef lower 95\\% &  coef upper 95\\% &  exp(coef) lower 95\\% &  exp(coef) upper 95\\% &  cmp to &     z &    p &  -log2(p) \\\\\n",
       "covariate &       &            &           &                 &                 &                      &                      &         &       &      &           \\\\\n",
       "\\midrule\n",
       "0         &  0.83 &       2.30 &      0.07 &            0.70 &            0.96 &                 2.02 &                 2.62 &    0.00 & 12.56 & 0.00 &    117.84 \\\\\n",
       "1         &  0.77 &       2.16 &      0.08 &            0.62 &            0.92 &                 1.86 &                 2.50 &    0.00 & 10.17 & 0.00 &     78.32 \\\\\n",
       "2         &  0.82 &       2.28 &      0.07 &            0.68 &            0.97 &                 1.98 &                 2.63 &    0.00 & 11.30 & 0.00 &     95.97 \\\\\n",
       "3         &  0.78 &       2.17 &      0.07 &            0.65 &            0.90 &                 1.91 &                 2.47 &    0.00 & 11.82 & 0.00 &    104.64 \\\\\n",
       "4         &  0.84 &       2.32 &      0.07 &            0.69 &            0.99 &                 2.00 &                 2.68 &    0.00 & 11.28 & 0.00 &     95.67 \\\\\n",
       "5         &  0.83 &       2.30 &      0.13 &            0.58 &            1.08 &                 1.78 &                 2.96 &    0.00 &  6.45 & 0.00 &     33.03 \\\\\n",
       "6         &  1.00 &       2.73 &      0.13 &            0.75 &            1.26 &                 2.11 &                 3.52 &    0.00 &  7.72 & 0.00 &     46.24 \\\\\n",
       "7         &  0.69 &       1.99 &      0.12 &            0.45 &            0.93 &                 1.57 &                 2.54 &    0.00 &  5.63 & 0.00 &     25.70 \\\\\n",
       "8         &  0.94 &       2.57 &      0.13 &            0.69 &            1.19 &                 2.00 &                 3.30 &    0.00 &  7.38 & 0.00 &     42.49 \\\\\n",
       "9         &  0.74 &       2.10 &      0.13 &            0.49 &            0.99 &                 1.64 &                 2.69 &    0.00 &  5.87 & 0.00 &     27.78 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 516 total observations, 216 right-censored observations>\n",
       "             duration col = 'time'\n",
       "                event col = 'event'\n",
       "      baseline estimation = breslow\n",
       "   number of observations = 516\n",
       "number of events observed = 300\n",
       "   partial log-likelihood = -1503.32\n",
       "         time fit was run = 2022-07-31 12:15:09 UTC\n",
       "\n",
       "---\n",
       "            coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\n",
       "covariate                                                                                                         \n",
       "0           0.83       2.30       0.07             0.70             0.96                 2.02                 2.62\n",
       "1           0.77       2.16       0.08             0.62             0.92                 1.86                 2.50\n",
       "2           0.82       2.28       0.07             0.68             0.97                 1.98                 2.63\n",
       "3           0.78       2.17       0.07             0.65             0.90                 1.91                 2.47\n",
       "4           0.84       2.32       0.07             0.69             0.99                 2.00                 2.68\n",
       "5           0.83       2.30       0.13             0.58             1.08                 1.78                 2.96\n",
       "6           1.00       2.73       0.13             0.75             1.26                 2.11                 3.52\n",
       "7           0.69       1.99       0.12             0.45             0.93                 1.57                 2.54\n",
       "8           0.94       2.57       0.13             0.69             1.19                 2.00                 3.30\n",
       "9           0.74       2.10       0.13             0.49             0.99                 1.64                 2.69\n",
       "\n",
       "            cmp to     z      p   -log2(p)\n",
       "covariate                                 \n",
       "0             0.00 12.56 <0.005     117.84\n",
       "1             0.00 10.17 <0.005      78.32\n",
       "2             0.00 11.30 <0.005      95.97\n",
       "3             0.00 11.82 <0.005     104.64\n",
       "4             0.00 11.28 <0.005      95.67\n",
       "5             0.00  6.45 <0.005      33.03\n",
       "6             0.00  7.72 <0.005      46.24\n",
       "7             0.00  5.63 <0.005      25.70\n",
       "8             0.00  7.38 <0.005      42.49\n",
       "9             0.00  5.87 <0.005      27.78\n",
       "---\n",
       "Concordance = 0.83\n",
       "Partial AIC = 3026.64\n",
       "log-likelihood ratio test = 485.26 on 10 df\n",
       "-log2(p) of ll-ratio test = 322.91"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cph = fit_cox(cases, subcohort,n_covariates)\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b3ca1b-eec0-4c39-9888-f12c09aff749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "from sksurv.util import Surv\n",
    "\n",
    "def concordance_score(n_covariates,test,model,censored = False,lifelines = False):\n",
    "    \n",
    "    X_test = test[range(0,n_covariates)]\n",
    "    \n",
    "    if censored == False:\n",
    "        event_times = test[\"y\"]\n",
    "        event_observed = test[\"event\"]\n",
    "    else:\n",
    "        event_times = test[\"time\"]\n",
    "        event_observed = test[\"event\"]\n",
    "    \n",
    "    if lifelines:\n",
    "        # test predictions\n",
    "        test_preds = model.predict_partial_hazard(X_test)\n",
    "        score = concordance_index(event_times, -1*test_preds, event_observed)\n",
    "    else:\n",
    "        y_test = Surv().from_arrays(event_observed,event_times)\n",
    "        score = model.score(X_test,y_test)\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d05338-0412-428a-9810-750c9c90daad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8707086899972778"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concordance_score(n_covariates,test,cph,lifelines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3664df-cc78-4ac0-aec0-35fb93cbb3d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'int_brier_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5dc7a2b689ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mint_brier_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubcohort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlifelines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'int_brier_score' is not defined"
     ]
    }
   ],
   "source": [
    "int_brier_score(cases,subcohort,test,cph,lifelines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368f1f7-2295-4c04-9e7e-881c52a174f1",
   "metadata": {},
   "source": [
    "## Weighted Cox Model\n",
    "Now we fit a Cox model using weighting methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e325a95-0b8a-4866-a859-05ea3f8929ee",
   "metadata": {},
   "source": [
    "### Barlow weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739da00d-c25b-4c84-89dd-c5db0e8ad5a9",
   "metadata": {},
   "source": [
    "To fit the model with Barlow weights, we split the data points corresponding to events into two parts. We use the case data to construct the interval at the event which has weight 1. Using the subcohort data, we construct the interval before the event that has weight $\\frac{1}{\\alpha}$. All non-events in the subcohort have weight $\\frac{1}{\\alpha}$ while in the risk set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482510ae-32bf-4078-8ab1-4d0f563e8282",
   "metadata": {},
   "source": [
    "Function for changing data for Cox model with Barlow weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb886aa-956a-4736-a927-382e98a91339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barlow_trans(cases,subcohort, n_covariates, alpha = len(subcohort)/len(cohort)):\n",
    "    # cases: cases dataframe\n",
    "    # subcohort: subcohort dataframe\n",
    "    # n_covariates: number of covariates used in the simulation\n",
    "    # alpha: the sampling proportion used for the subcohort\n",
    "    \n",
    "    \n",
    "    # finding the order of magnitude of data to pick the appropriate size of each \"instant\". We use the largest event time for this.\n",
    "    order = int(np.floor(np.log(max(cases[\"time\"]))/np.log(10))) \n",
    "    \n",
    "    \n",
    "    cases = cases.assign(\n",
    "        # setting events outside subcohort to start just before they occur\n",
    "        start_time = lambda df: df[\"time\"] - 10**-(- order + 5),\n",
    "        # adding appropriate weight\n",
    "        weight = 1,\n",
    "        subcohort = False\n",
    "    )\n",
    "    # setting times < 0  to 0\n",
    "    cases[\"start_time\"] = np.where(cases[\"start_time\"] < 0, 0, cases[\"start_time\"]) \n",
    "    \n",
    "    subcohort = subcohort.assign(\n",
    "        # if it is a case, the weight should be the same as the subcohort until close to the time of the event. \n",
    "        time = lambda df: np.where(df[\"event\"], df[\"time\"] - 10**-(- order + 5), df[\"time\"]), \n",
    "        # the events start from the origin\n",
    "        start_time = 0, \n",
    "        event = False,\n",
    "        weight = 1/alpha,\n",
    "        subcohort = True\n",
    "    )\n",
    "    # drop any rows where the start time in cases is 0.\n",
    "    if len(np.where(cases[\"start_time\"] == 0)) == 0:\n",
    "        subcohort.drop(np.where(cases[\"start_time\"] == 0))\n",
    "\n",
    "    return(pd.concat([cases,subcohort])[[i for i in range(0,n_covariates)]+[\"start_time\",\"time\", \"event\",\"weight\",\"subcohort\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728404e-eac8-49aa-a1e6-7dacfbcd6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cox_barlow(cases, subcohort,n_covariates):\n",
    "    case_subcohort_df = barlow_trans(cases,subcohort,n_covariates,len(subcohort)/len(cohort)).drop(columns = \"subcohort\")\n",
    "    \n",
    "    # creating the model and fitting the data\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(case_subcohort_df, entry_col = \"start_time\", duration_col = \"time\",event_col = \"event\",weights_col = \"weight\",robust = True)\n",
    "    return(cph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf2ddf-71f1-4c4d-ba99-bab03cd4da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph2 = fit_cox_barlow(cases,subcohort,n_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc95376-c74c-4cb6-8c23-31ea8b49022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph2.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808cf77-e84e-42b8-aa8d-7b87187b2b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = cph2.predict_partial_hazard(test[range(0,10)])\n",
    "test_preds\n",
    "event_times = test[\"time\"]\n",
    "event_observed = test[\"event\"]\n",
    "event_times, event_observed\n",
    "from lifelines.utils import concordance_index\n",
    "concordance_index(event_times, -test_preds, event_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac930405-3c97-407f-b0a1-17e496adbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,cph2,lifelines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b281a-9c5d-40ed-9c2c-f7ecb99f2dac",
   "metadata": {},
   "source": [
    "### Prentice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009750c-5a4a-4af9-addf-cb4eff4a7dd6",
   "metadata": {},
   "source": [
    "To fit the model with Prentice weights, we split the data points corresponding to events into two parts. We use the case data to construct the interval at the event which has weight 1. Using the subcohort data, we construct the interval before the event that has weight $1$. All non-events in the subcohort have weight $1$ while in the risk set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2bd981-4ea9-4998-9e00-6ba574a5fe93",
   "metadata": {},
   "source": [
    "Function for changing data for Cox model with Prentice weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da612662-0b28-49b4-b20c-d410e09cbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prentice_trans(cases,subcohort,n_covariates):\n",
    "    # finding the order of magnitude of data to pick the appropriate size of each \"instant\". We use the largest event time for this.\n",
    "    order = int(np.floor(np.log(max(cases[\"time\"]))/np.log(10))) \n",
    "    \n",
    "    \n",
    "    cases = cases.assign(\n",
    "        # rounding all of the \n",
    "#         time = round(cases[\"time\"],- order + 5),\n",
    "        # setting events outside subcohort to start just before they occur\n",
    "        start_time = lambda df: df[\"time\"] - 10**-(- order + 5),\n",
    "        # adding appropriate weight\n",
    "        weight = 1,\n",
    "        subcohort = False\n",
    "    )\n",
    "    #filtering out readings with negative start times\n",
    "    cases = cases.query(\"start_time > 0\") \n",
    "    \n",
    "    subcohort = subcohort.assign(\n",
    "        # if it is a case, the weight should be the same as the subcohort until close to the time of the event. \n",
    "        time = lambda df: np.where(df[\"event\"], df[\"time\"] - 10**-(- order + 5), df[\"time\"]), \n",
    "        # the events start from the origin\n",
    "        start_time = 0, \n",
    "        event = False,\n",
    "        weight = 1,\n",
    "        subcohort = True\n",
    "    ) \n",
    "\n",
    "    return(pd.concat([cases,subcohort])[[i for i in range(0,n_covariates)]+[\"start_time\",\"time\", \"event\",\"weight\",\"subcohort\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d516a39-456e-4f1f-9c73-0087d234e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cox_prentice(cases, subcohort,n_covariates):\n",
    "    case_subcohort_df = prentice_trans(cases,subcohort,n_covariates).drop(columns = \"subcohort\")\n",
    "    \n",
    "    # creating the model and fitting the data\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(case_subcohort_df, entry_col = \"start_time\", duration_col = \"time\",event_col = \"event\",weights_col = \"weight\",robust = True)\n",
    "    return(cph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fdd63-6b12-40ed-af31-ba772855483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph3 = fit_cox_prentice(cases,subcohort,n_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76d1f0-aa2a-4af4-9198-4fc47876cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph3.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736704af-5bc1-4818-afc5-29cb5d47be16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = cph3.predict_partial_hazard(test[range(0,10)])\n",
    "test_preds\n",
    "event_times = test[\"time\"]\n",
    "event_observed = test[\"event\"]\n",
    "event_times, event_observed\n",
    "from lifelines.utils import concordance_index\n",
    "concordance_index(event_times, -test_preds, event_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e36ac4-501a-4510-a053-9546f1e9dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,cph3,lifelines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939b8df-efcd-45ec-b04e-497225260125",
   "metadata": {},
   "source": [
    "### Self and Prentice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845bb45-91a1-40ba-8256-2cc3502ab325",
   "metadata": {},
   "source": [
    "To fit the model with Self and Prentice weights, we split the data points corresponding to events into two parts. Case data has weight $0$ in the pseudo-partial likelihood. The Cox fitter does not support settingn weight to 0, so let the weight be extremely small. Subcohort data has weight $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85993ec-bd4c-43d7-ac1b-b4addff6eda6",
   "metadata": {},
   "source": [
    "Function for changing data for Cox model with Prentice weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5b474-b151-4d34-bb39-ec196d004334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_prentice_trans(cases,subcohort,n_covariates):\n",
    "    # finding the order of magnitude of data to pick the appropriate size of each \"instant\". We use the largest event time for this.\n",
    "    order = int(np.floor(np.log(max(cases[\"time\"]))/np.log(10))) \n",
    "    \n",
    "    # removing the cases that are in the subcohort from the cases data frame\n",
    "    cases = cases[~cases.index.isin(subcohort.index)]\n",
    "    # Adding the non-subcohort case weights\n",
    "    cases[\"weight\"] = 10**(-order - 5)\n",
    "    cases[\"subcohort\"] = False\n",
    "    \n",
    "    subcohort = subcohort.assign(\n",
    "        weight = 1,\n",
    "        subcohort = True\n",
    "    )\n",
    "\n",
    "    return(pd.concat([cases,subcohort])[[i for i in range(0,n_covariates)]+[\"time\", \"event\",\"weight\",\"subcohort\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb5a8e-a167-46e7-b432-a764e8d90a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_prentice_trans(cases,subcohort,n_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710b309-7762-4e7a-b63e-fb78ed66c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cox_self_prentice(cases, subcohort):\n",
    "    case_subcohort_df = self_prentice_trans(cases,subcohort,n_covariates).drop(columns = \"subcohort\")\n",
    "    \n",
    "    # creating the model and fitting the data\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(case_subcohort_df, duration_col = \"time\",event_col = \"event\",weights_col = \"weight\",robust = True)\n",
    "    return(cph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532d8af-e652-4227-961f-672652032dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph4 = fit_cox_self_prentice(cases,subcohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdda8b2-153b-413d-8839-df186b26d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph4.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6efdb-8053-4932-b668-d6ea3612e661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = cph4.predict_partial_hazard(test[range(0,10)])\n",
    "test_preds\n",
    "event_times = test[\"time\"]\n",
    "event_observed = test[\"event\"]\n",
    "event_times, event_observed\n",
    "from lifelines.utils import concordance_index\n",
    "concordance_index(event_times, -test_preds, event_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571bc8e-59f5-4a8f-8077-fc6d0739842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,cph4,lifelines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35aee7a-60f6-40c8-8b39-829464847f4e",
   "metadata": {},
   "source": [
    "## Penalised Cox Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f25ed-af85-49c5-9c28-501890523044",
   "metadata": {},
   "source": [
    "For simplicity, consider L1, L2 and 0.5 L1 weight in the penality function. We use k-fold cross validation to find the optimal $\\alpha$. We need to adapt the inbuilt `k_fold_cross_validation` function to accomodate changing the dataset for the time dependent weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd20935-09b4-4261-8b41-4dd9ae7afcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cox_k_fold import cox_k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79e36a-b5a8-44b9-8003-da18f7c6725d",
   "metadata": {},
   "outputs": [],
   "source": [
    " cox_k_fold(\n",
    "    cph2, cases, subcohort,n_covariates, barlow_trans, \"time\", event_col=\"event\", k=5, scoring_method=\"log_likelihood\", fitter_kwargs={\"weights_col\": \"weight\", \"robust\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf2667-13a4-4a33-b7be-09e2132e3c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pen_cox_barlow(cases, subcohort,n_covariates, l1_ratio = 0, penalizer_show = False):\n",
    "    # choosing the penaliser\n",
    "    avg_score = []\n",
    "    for penalizer in range(0,20):\n",
    "        score = cox_k_fold(CoxPHFitter(penalizer = penalizer/10),cases, subcohort,n_covariates, barlow_trans,\"time\", event_col=\"event\", k=5, scoring_method=\"log_likelihood\", fitter_kwargs={\"weights_col\": \"weight\", \"robust\": True})\n",
    "        avg_score.append(np.mean(score))\n",
    "    penalizer = int(np.where(avg_score == max(avg_score))[0])/10\n",
    "    \n",
    "    # creating the model and fitting the data\n",
    "    cph = CoxPHFitter(penalizer = penalizer,l1_ratio = l1_ratio)\n",
    "    case_subcohort_df = barlow_trans(cases,subcohort,n_covariates).drop(columns = \"subcohort\")\n",
    "    cph.fit(case_subcohort_df, entry_col = \"start_time\", duration_col = \"time\",event_col = \"event\",weights_col = \"weight\",robust = True)\n",
    "    if penalizer_show:\n",
    "        return(cph, penalizer)\n",
    "    else:\n",
    "        return(cph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a5f6b-6b7d-4a1b-b070-3c7be8b5188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph5, penalizer = fit_pen_cox_barlow(cases,subcohort,n_covariates, l1_ratio = 0, penalizer_show = True)\n",
    "print(penalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fcefd6-44aa-4cd3-8b09-0d5c9f93279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph5.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce6bb0-3cdb-4788-9be0-019b35329001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = cph5.predict_partial_hazard(test[range(0,10)])\n",
    "test_preds\n",
    "event_times = test[\"time\"]\n",
    "event_observed = test[\"event\"]\n",
    "event_times, event_observed\n",
    "from lifelines.utils import concordance_index\n",
    "concordance_index(event_times, -test_preds, event_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37f244-4fc5-4ae2-9fa0-34c24491b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,cph5,lifelines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f1b8f-f6c7-4c89-aba1-1faec8712a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pen_cox_prentice(cases, subcohort,n_covariates, l1_ratio = 0, penalizer_show = False):\n",
    "    # choosing the penaliser\n",
    "    avg_score = []\n",
    "    for penalizer in range(0,20):\n",
    "        score = cox_k_fold(CoxPHFitter(penalizer = penalizer/10),cases, subcohort,n_covariates, prentice_trans,\"time\", event_col=\"event\", k=5, scoring_method=\"log_likelihood\", fitter_kwargs={\"weights_col\": \"weight\", \"robust\": True})\n",
    "        avg_score.append(np.mean(score))\n",
    "    penalizer = int(np.where(avg_score == max(avg_score))[0])/10\n",
    "    \n",
    "    # creating the model and fitting the data\n",
    "    cph = CoxPHFitter(penalizer = penalizer,l1_ratio = l1_ratio)\n",
    "    case_subcohort_df = prentice_trans(cases,subcohort,n_covariates).drop(columns = \"subcohort\")\n",
    "    cph.fit(case_subcohort_df, entry_col = \"start_time\", duration_col = \"time\",event_col = \"event\",weights_col = \"weight\",robust = True)\n",
    "    if penalizer_show:\n",
    "        return(cph, penalizer)\n",
    "    else:\n",
    "        return(cph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014390d5-eb8c-484a-8129-ee1017ad8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph6, penalizer = fit_pen_cox_prentice(cases,subcohort,n_covariates, l1_ratio = 0,penalizer_show = True)\n",
    "print(penalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08add05-d28e-427d-b3ab-90535c7dfde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph6.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4b81d4-ed19-4571-b754-2dfa7ea054be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = cph6.predict_partial_hazard(test[range(0,10)])\n",
    "test_preds\n",
    "event_times = test[\"time\"]\n",
    "event_observed = test[\"event\"]\n",
    "event_times, event_observed\n",
    "from lifelines.utils import concordance_index\n",
    "concordance_index(event_times, -test_preds, event_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72c480-a89e-480a-8b0a-1596d830a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,cph6,lifelines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d18fc-0f45-4e74-ba27-54433d395664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pen_cox_self_prentice(cases, subcohort,n_covariates, l1_ratio = 0, penalizer_show = False):\n",
    "    # choosing the penaliser\n",
    "    avg_score = []\n",
    "    for penalizer in range(0,20):\n",
    "        score = cox_k_fold(CoxPHFitter(penalizer = penalizer/10),cases, subcohort,n_covariates, self_prentice_trans,\"time\", event_col=\"event\", k=5, scoring_method=\"log_likelihood\", fitter_kwargs={\"weights_col\": \"weight\", \"robust\": True})\n",
    "        avg_score.append(np.mean(score))\n",
    "    penalizer = int(np.where(avg_score == max(avg_score))[0])/10\n",
    "    \n",
    "    # creating the model and fitting the data\n",
    "    cph = CoxPHFitter(penalizer = penalizer,l1_ratio = l1_ratio)\n",
    "    case_subcohort_df = self_prentice_trans(cases,subcohort,n_covariates).drop(columns = \"subcohort\")\n",
    "    cph.fit(case_subcohort_df, duration_col = \"time\",event_col = \"event\",weights_col = \"weight\",robust = True)\n",
    "    if penalizer_show:\n",
    "        return(cph, penalizer)\n",
    "    else:\n",
    "        return(cph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05956702-e61c-479c-93a3-50a168190f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph7, penalizer = fit_pen_cox_self_prentice(cases,subcohort,n_covariates, l1_ratio = 0,penalizer_show = True)\n",
    "print(penalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad842dd-5736-418b-ad01-670fe8eb6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph7.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f868fe9-e1bb-4861-8ea3-ab9cfca9b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = cph7.predict_partial_hazard(test[range(0,10)])\n",
    "test_preds\n",
    "event_times = test[\"time\"]\n",
    "event_observed = test[\"event\"]\n",
    "event_times, event_observed\n",
    "from lifelines.utils import concordance_index\n",
    "concordance_index(event_times, -test_preds, event_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a6ecf-496e-47e0-8724-9421dbd07061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_brier_score(cases,subcohort,test,n_covariates,model,lifelines = False):\n",
    "    # First we get a copy of the training data to estimate the censoring distribution\n",
    "    # creating case-subcohort data frame and removing duplicate entries of cases\n",
    "    case_subcohort = pd.concat([cases,subcohort])\n",
    "    case_subcohort = case_subcohort.drop(columns = 'subcohort').drop_duplicates()\n",
    "\n",
    "    # oversampled data set\n",
    "    # \"covariates\"\n",
    "    X = case_subcohort[[i for i in range(0,n_covariates)]+['time']]\n",
    "    # \"classes\" to be oversampled. Here, cases\n",
    "    y = case_subcohort[\"event\"]\n",
    "    ros = RandomOverSampler(sampling_strategy = {True: len(cases), False: len(cohort) - len(cases)})\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    y_train = Surv().from_arrays(y_resampled, X_resampled['time'])\n",
    "    \n",
    "    if lifelines == False:\n",
    "        # survival function predictions\n",
    "        survs = model.predict_survival_function(X_test)\n",
    "\n",
    "        # times at which to evaluate survival function\n",
    "        times = np.arange(min(model.event_times_),max(model.event_times_),(max(model.event_times_) - min(model.event_times_))/100)\n",
    "\n",
    "        preds = np.asarray([[fn(t) for t in times] for fn in survs])\n",
    "\n",
    "        score = integrated_brier_score(y_train, y_test, preds, times)\n",
    "    else:\n",
    "        # survival function predictions\n",
    "        survs = model.predict_survival_function(X_test)\n",
    "\n",
    "        # times at which to evaluate survival function\n",
    "        times = survs.index[np.where((survs.index < max(test['time'])) & (survs.index > min(test['time']))) ]\n",
    "\n",
    "        preds = np.array(survs.iloc[np.where((survs.index < max(test['time'])) & (survs.index > min(test['time']))) ]).transpose()\n",
    "\n",
    "        score = integrated_brier_score(y_train, y_test, preds, times)\n",
    "        \n",
    "    return(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68cbaa-48a1-4370-acca-c4364abee764",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,cph7,lifelines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6109e74-8921-4a5a-b647-4c33e8fdfaa4",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72b3b0-9a75-4d67-ac03-0a56a8e52e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.tree import SurvivalTree\n",
    "from sksurv.util import Surv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9fba2-622c-4f05-96f5-48123e5b0825",
   "metadata": {},
   "source": [
    "### No weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e16ec-f1cb-455e-aee6-7b03f7be2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweighted_tree(cases,subcohort,n_covariates):\n",
    "    # creating case-subcohort data frame and removing duplicate entries of cases\n",
    "    case_subcohort = pd.concat([cases,subcohort])\n",
    "    case_subcohort = case_subcohort.drop(columns = 'subcohort').drop_duplicates()\n",
    "    \n",
    "    # matrix of covariates\n",
    "    X_train = case_subcohort[range(0,n_covariates)]\n",
    "    # (event,time) response array\n",
    "    y_train = Surv().from_arrays(case_subcohort['event'], case_subcohort['time'])\n",
    "    \n",
    "    # fitting the tree\n",
    "    tree = SurvivalTree()\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22412f-a093-443e-9843-24967168f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = unweighted_tree(cases,subcohort,n_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1f8b8-7aa3-4b59-98e7-8a288163b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[range(0,10)]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee775668-46b0-4f07-8763-f3a423474b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = Surv().from_dataframe('event','time',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f248df-7773-4dc3-b34e-9ce777c9ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc76e68-4cbf-4fef-a625-d6245e009768",
   "metadata": {},
   "source": [
    "### Random over sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc7cc3-1d07-497e-937e-6559abddc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8952a74-0c81-4184-b968-10a97781a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ros_tree(cases,subcohort,n_covariates):\n",
    "    # creating case-subcohort data frame and removing duplicate entries of cases\n",
    "    case_subcohort = pd.concat([cases,subcohort])\n",
    "    case_subcohort = case_subcohort.drop(columns = 'subcohort').drop_duplicates()\n",
    "    \n",
    "    # oversampled data set\n",
    "    # \"covariates\"\n",
    "    X = case_subcohort[[i for i in range(0,n_covariates)]+['time']]\n",
    "    # \"classes\" to be oversampled. Here, cases\n",
    "    y = case_subcohort[\"event\"]\n",
    "    ros = RandomOverSampler(sampling_strategy = {True: len(cases), False: len(cohort) - len(cases)})\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    \n",
    "    # matrix of covariates\n",
    "    X_train = X_resampled[range(0,n_covariates)]\n",
    "    # (event,time) response array\n",
    "    y_train = Surv().from_arrays(y_resampled, X_resampled['time'])\n",
    "    \n",
    "    # fitting the tree\n",
    "    tree = SurvivalTree()\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09d1d5-3cf1-4a4c-919c-2a4862576357",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ros_tree(cases,subcohort,n_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e68d25-b021-4b18-a1f9-19320e674552",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[range(0,10)]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d750e-787d-4a8f-9f41-e7c4a69b22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = Surv().from_dataframe('event','time',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd59dba-eba7-49a9-9da1-b6a0564e4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd87c0-0b96-44ec-80fe-9a54b2e25f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e3b44-46e0-4422-ad81-926e7a73d97a",
   "metadata": {},
   "source": [
    "### SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb7134-7aad-4086-b7c5-d1c754b80e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9efee-8f8c-440a-9f8a-68ae7e1a6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smotenc_tree(cases,subcohort,n_covariates):\n",
    "    case_subcohort = pd.concat([cases,subcohort])\n",
    "    case_subcohort = case_subcohort.drop(columns = 'subcohort').drop_duplicates()\n",
    "\n",
    "    # oversampled data set\n",
    "    # \"covariates\"\n",
    "    X = case_subcohort[[i for i in range(0,n_covariates)]+['time']]\n",
    "    # \"classes\" to be oversampled. Here, cases\n",
    "    y = case_subcohort[\"event\"]\n",
    "    categorical_features = list(np.where([sum(~(cases[i].isin([0,1]))) == 0 for i in range(0,n_covariates)])[0])\n",
    "    smote_nc = SMOTENC(categorical_features=categorical_features)\n",
    "    X_resampled, y_resampled = smote_nc.fit_resample(X, y)\n",
    "    \n",
    "    # matrix of covariates\n",
    "    X_train = X_resampled[range(0,n_covariates)]\n",
    "    # (event,time) response array\n",
    "    y_train = Surv().from_arrays(y_resampled, X_resampled['time'])\n",
    "    \n",
    "    # fitting the tree\n",
    "    tree = SurvivalTree()\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    return(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d70a5-796a-4928-bfc6-816245c45cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2 = smotenc_tree(cases,subcohort,n_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80f77c-7847-4dc6-82c8-aafda69cd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced26a40-e323-423d-9362-fd8fee505778",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d33c9-62e8-404d-8a2f-f0831798ebbb",
   "metadata": {},
   "source": [
    "## Balanced Survival Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "042c72cd-86a0-47b5-93fe-4c9b59ac73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfbaca8-ce8a-4abd-8763-cc8b40dd15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweighted_rsf(cases,subcohort,n_covariates):\n",
    "    # creating case-subcohort data frame and removing duplicate entries of cases\n",
    "    case_subcohort = pd.concat([cases,subcohort])\n",
    "    case_subcohort = case_subcohort.drop(columns = 'subcohort').drop_duplicates()\n",
    "    \n",
    "    # matrix of covariates\n",
    "    X_train = case_subcohort[range(0,n_covariates)]\n",
    "    # (event,time) response array\n",
    "    y_train = Surv().from_arrays(case_subcohort['event'], case_subcohort['time'])\n",
    "    \n",
    "    # fitting the random survival forest\n",
    "    rsf = RandomSurvivalForest(n_estimators=1000)\n",
    "    rsf.fit(X_train, y_train)\n",
    "    \n",
    "    return(rsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5915f87d-f388-4568-945c-633ec935931c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['subcohort'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-293fa4f24f22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrsf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munweighted_rsf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubcohort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_covariates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-f52661f8d205>\u001b[0m in \u001b[0;36munweighted_rsf\u001b[1;34m(cases, subcohort, n_covariates)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# creating case-subcohort data frame and removing duplicate entries of cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcase_subcohort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubcohort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcase_subcohort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcase_subcohort\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'subcohort'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# matrix of covariates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4306\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4307\u001b[0m         \"\"\"\n\u001b[1;32m-> 4308\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4309\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4153\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4188\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5589\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5590\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5591\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['subcohort'] not found in axis\""
     ]
    }
   ],
   "source": [
    "rsf = unweighted_rsf(cases,subcohort,n_covariates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9c086-c332-4a2d-be83-e1b3b2536e0e",
   "metadata": {},
   "source": [
    "### Naive random over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0114b-4482-48dd-a20f-ab6213904198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f3645-c1c1-4caa-9c87-dc16a25c6c34",
   "metadata": {},
   "source": [
    "We set the \"class\" to event, because controls are undersampled. We want there to be similar to in the cohort, so we want the number of cases simply to be $n_\\text{cases}$, and the number of controls to be $n_\\text{cohort} - n_\\text{controls}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1bfa82-9ee7-4b8b-8eba-1bc5ece8cafb",
   "metadata": {},
   "source": [
    "Function fitting random oversampled random survival forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6e60d-0beb-4132-a228-e72022ab3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[range(0,n_covariates)]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726f27f-c2a4-4c77-896a-3f7bea974b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = Surv().from_dataframe('event','time',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e294351-2dd3-4c97-892e-06e9a1e9f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668a2c7-3788-4543-99da-ad4f44c9f749",
   "metadata": {},
   "source": [
    "So concordance not quite as good here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb9eb8-cba8-4e22-8b8e-2bad56a6a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import integrated_brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c7732-4b2d-422a-8413-d2a560674717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_brier_score(cases,subcohort,test,model):\n",
    "    # First we get a copy of the training data to estimate the censoring distribution\n",
    "    # creating case-subcohort data frame and removing duplicate entries of cases\n",
    "    case_subcohort = pd.concat([cases,subcohort])\n",
    "    case_subcohort = case_subcohort.drop(columns = 'subcohort').drop_duplicates()\n",
    "\n",
    "    # oversampled data set\n",
    "    # \"covariates\"\n",
    "    X = case_subcohort[[i for i in range(0,n_covariates)]+['time']]\n",
    "    # \"classes\" to be oversampled. Here, cases\n",
    "    y = case_subcohort[\"event\"]\n",
    "    ros = RandomOverSampler(sampling_strategy = {True: len(cases), False: len(cohort) - len(cases)})\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    y_train = Surv().from_arrays(y_resampled, X_resampled['time'])\n",
    "    \n",
    "    # survival function predictions\n",
    "    survs = model.predict_survival_function(X_test)\n",
    "    \n",
    "    # times at which to evaluate survival function\n",
    "    times = np.arange(min(model.event_times_),max(model.event_times_),(max(model.event_times_) - min(model.event_times_))/100)\n",
    "    \n",
    "    preds = np.asarray([[fn(t) for t in times] for fn in survs])\n",
    "    \n",
    "    score = integrated_brier_score(y_train, y_test, preds, times)\n",
    "    \n",
    "    return(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c557507-94f6-4576-b067-2e4fb9d77b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,rsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb13a2b-be69-4ff9-8dbe-846b5fcff106",
   "metadata": {},
   "source": [
    "### SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3dda7-cad3-450d-b892-5e1be78cc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f5363-9abd-4553-9f70-522dc486a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smotenc_rsf(cases,subcohort,n_covariates):\n",
    "    case_subcohort = pd.concat([cases,subcohort])\n",
    "    case_subcohort = case_subcohort.drop(columns = 'subcohort').drop_duplicates()\n",
    "\n",
    "    # oversampled data set\n",
    "    # \"covariates\"\n",
    "    X = case_subcohort[[i for i in range(0,n_covariates)]+['time']]\n",
    "    # \"classes\" to be oversampled. Here, cases\n",
    "    y = case_subcohort[\"event\"]\n",
    "    categorical_features = list(np.where([sum(~(cases[i].isin([0,1]))) == 0 for i in range(0,10)])[0])\n",
    "    smote_nc = SMOTENC(categorical_features=categorical_features)\n",
    "    X_resampled, y_resampled = smote_nc.fit_resample(X, y)\n",
    "    \n",
    "    # matrix of covariates\n",
    "    X_train = X_resampled[range(0,n_covariates)]\n",
    "    # (event,time) response array\n",
    "    y_train = Surv().from_arrays(y_resampled, X_resampled['time'])\n",
    "    \n",
    "    # fitting the random survival forest\n",
    "    rsf = RandomSurvivalForest(n_estimators=1000)\n",
    "    rsf.fit(X_train, y_train)\n",
    "    \n",
    "    return(rsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5f591-c605-4ba5-b319-1f1e9f7d2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf2 = smotenc_rsf(cases,subcohort,n_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6a124-69d1-4013-8e7d-3f001af4bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320a22a-1704-46b4-af56-e9da1ab8a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_brier_score(cases,subcohort,test,rsf2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
