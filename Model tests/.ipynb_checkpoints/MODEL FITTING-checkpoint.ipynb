{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8cb9956-c9bd-4687-96be-07a3537c4dda",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edde14a6-07f8-4d6f-97be-f05058f505ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import weibull_min # r weibull simulation\n",
    "from scipy.stats import norm # for covariate simulation\n",
    "from scipy.stats import gamma # for weibull shape parameter\n",
    "from scipy.stats import bernoulli # for censoring\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8a6201-1f25-461f-8424-1d00cedccb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_simulation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211b3c5-89fc-4576-b558-f1bdafd2d5b5",
   "metadata": {},
   "source": [
    "## Cox mode, unweighted\n",
    "First we look at the standard Cox model without adjusting for bias. When the model is not mispecified at all, the performance of the unweighted version may not be that different, however, if the model is mispecified, the results may be poorer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5eea18-912a-450a-ad1e-8cd08fb355c1",
   "metadata": {},
   "source": [
    "The following function fits an unweighted Cox model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae66718-4a3c-4113-a781-0cbf4f3bc7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "def fit_cox(cases, subcohort,n_covariates):\n",
    "    # cases: cases dataframe\n",
    "    # cohort: cohort dataframe\n",
    "    # n_covariates: the number of covariates used in the simulation\n",
    "    \n",
    "    # creating a single case subcohort dataframe\n",
    "    case_subcohort_df = pd.concat([cases,subcohort])\n",
    "    # removing unnecessary columns and duplicate rows\n",
    "    case_subcohort_df = case_subcohort_df.loc[case_subcohort_df.duplicated() == False,[i for i in range(0,n_covariates)]+[\"time\", \"event\"]]\n",
    "    \n",
    "    # creating the model and fitting the data\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(case_subcohort_df, duration_col = \"time\", event_col = \"event\")\n",
    "    return(cph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e35e8-5a72-475d-8911-1acf1bab7f15",
   "metadata": {},
   "source": [
    "## Weighted Cox Model\n",
    "Now we fit a Cox model using weighting methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2af40-6596-4284-911d-910f32bc2221",
   "metadata": {},
   "source": [
    "To fit the model with Barlow weights, we split the data points corresponding to events into two parts. We use the case data to construct the interval at the event which has weight 1. Using the subcohort data, we construct the interval before the event that has weight $\\frac{1}{\\alpha}$. All non-events in the subcohort have weight $\\frac{1}{\\alpha}$ while in the risk set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f436d45-a645-4d80-8a6a-b0a43a8f56f0",
   "metadata": {},
   "source": [
    "Function for changing data for Cox model with Barlow weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "81fb6137-7ae5-47e7-b8af-0d7ca566fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barlow_trans(cases,subcohort, n_covariates, alpha = len(subcohort)/len(cohort)):\n",
    "    # cases: cases dataframe\n",
    "    # subcohort: subcohort dataframe\n",
    "    # n_covariates: number of covariates used in the simulation\n",
    "    # alpha: the sampling proportion used for the subcohort\n",
    "    \n",
    "    \n",
    "    # finding the order of magnitude of data to pick the appropriate size of each \"instant\". We use the largest event time for this.\n",
    "    order = int(np.floor(np.log(max(cases[\"time\"]))/np.log(10))) \n",
    "    \n",
    "    \n",
    "    cases = cases.assign(\n",
    "        # setting events outside subcohort to start just before they occur\n",
    "        start_time = lambda df: df[\"time\"] - 10**-(- order + 5),\n",
    "        # adding appropriate weight\n",
    "        weight = 1,\n",
    "        subcohort = False\n",
    "    )\n",
    "    # setting times < 0  to 0\n",
    "    cases[\"start_time\"] = np.where(cases[\"start_time\"] < 0, 0, cases[\"start_time\"]) \n",
    "    \n",
    "    subcohort = subcohort.assign(\n",
    "        # if it is a case, the weight should be the same as the subcohort until close to the time of the event. \n",
    "        time = lambda df: np.where(df[\"event\"], df[\"time\"] - 10**-(- order + 5), df[\"time\"]), \n",
    "        # the events start from the origin\n",
    "        start_time = 0, \n",
    "        event = False,\n",
    "        weight = 1/alpha,\n",
    "        subcohort = True\n",
    "    )\n",
    "    # drop any rows where the start time in cases is 0.\n",
    "    if len(np.where(cases[\"start_time\"] == 0)) == 0:\n",
    "        subcohort.drop(np.where(cases[\"start_time\"] == 0))\n",
    "\n",
    "    return(pd.concat([cases,subcohort])[[i for i in range(0,n_covariates)]+[\"start_time\",\"time\", \"event\",\"weight\",\"subcohort\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
